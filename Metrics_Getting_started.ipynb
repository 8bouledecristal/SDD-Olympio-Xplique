{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTLKhCaktZxn"
      },
      "source": [
        "# üëã Welcome to the **Attribution Metrics** tutorial\n",
        "\n",
        "\n",
        "In this tutorial, we will take a model, several images and run several **Attribution methods** to compare them **quantitatively** using Metrics.\n",
        "\n",
        "We will download images from tiny-imagenet-200 dataset and compute explanations using a pre-trained model. Once the explanations are computed, we can test their **Faithfullness** using one of the available metrics: `Deletion`,`Insertion` or `MuFidelity`. Another property of a good explanation is **Stability**: for this we can use the `AverageStability` metric.\n",
        "\n",
        "üí° In order to better understand these metrics, I put here some of the references:\n",
        "[Deletion & Insertion](https://arxiv.org/pdf/1806.07421.pdf), [MuFidelity, AverageStability](https://arxiv.org/pdf/2005.00631.pdf).\n",
        "\n",
        "_Author: [Thomas FEL](https://thomasfel.fr/)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zupvVHHmK43"
      },
      "source": [
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/deel-ai/xplique/blob/master/docs/assets/banner.png?raw=true\"/>\n",
        "</p>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "- üìò <a href=\"https://deel-ai.github.io/xplique\"> Documentation </a>\n",
        "- <img src=\"https://github.githubassets.com/images/icons/emoji/octocat.png\" width=20> <a href=\"https://github.com/deel-ai/xplique\"> Github </a>\n",
        "- ü¶ä <a href=\"https://github.com/deel-ai/xplique/blob/master/TUTORIALS.md\"> More Xplique tutorials </a>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9fX4J6_R6Mb"
      },
      "source": [
        "# Installing **Xplique** and utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PxI4ALrVi_CB"
      },
      "outputs": [],
      "source": [
        "!pip install -q xplique\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "\n",
        "import xplique\n",
        "from xplique.plots import plot_attributions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uhG_cAj0ZcU"
      },
      "source": [
        "#¬†üêß Download and load some Penguins images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwCEIPtmzNO6",
        "outputId": "b98060c8-a410-4992-9022-8086903b840d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   253  100   253    0     0    282      0 --:--:-- --:--:-- --:--:--   282\n",
            "[./tiny-imagenet-200]\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of ./tiny-imagenet-200 or\n",
            "        ./tiny-imagenet-200.zip, and cannot find ./tiny-imagenet-200.ZIP, period.\n"
          ]
        }
      ],
      "source": [
        "!curl -O 'http://cs231n.stanford.edu/tiny-imagenet-200'\n",
        "!unzip -qq ./tiny-imagenet-200\n",
        "!rm tiny-imagenet-200\n",
        "# !cd sample_data\n",
        "# !pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "loHWTeT9zaDc",
        "outputId": "5050b3a0-1f1a-4619-cf66-02fab5be5a50"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'tiny-imagenet-200/train/n02056570/images'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-1ca9b4620dfd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimages_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'tiny-imagenet-200/train/n02056570/images'\u001b[0m \u001b[0;31m# king penguin class :)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mimages_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34mf\"{images_dir}/{p}\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnb_samples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'tiny-imagenet-200/train/n02056570/images'"
          ]
        }
      ],
      "source": [
        "nb_samples = 50\n",
        "input_size = 224\n",
        "\n",
        "images_dir = 'tiny-imagenet-200/train/n02056570/images' # king penguin class :)\n",
        "\n",
        "images_path = [f\"{images_dir}/{p}\" for p in os.listdir(images_dir)][:nb_samples]\n",
        "images = [cv2.imread(p)[...,::-1] for p in images_path]\n",
        "images = tf.image.resize(images, (input_size, input_size))\n",
        "images.shape\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = [15, 5]\n",
        "for i in range(10):\n",
        "  plt.subplot(1, 10, i+1)\n",
        "  plt.imshow(images[i]/255.0)\n",
        "  plt.axis('off')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLisQMes3oHD"
      },
      "outputs": [],
      "source": [
        "import tensorflow.keras.applications as app\n",
        "\n",
        "model, preprocessing = app.MobileNetV2(), app.mobilenet_v2.preprocess_input\n",
        "\n",
        "X = preprocessing(np.array(images, copy=True))\n",
        "Y = tf.one_hot([145] * len(X), 1000) # class 145 is king penguin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Awl3I5QcXhiD"
      },
      "source": [
        "#¬†üîç Generate explanations\n",
        "\n",
        "üí° To explain the logits is to explain the class, to explain the softmax is to explain why this class rather than another. It is thus recommended to remove the softmax activation to explain the logit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EcH3JW64Xuaj"
      },
      "outputs": [],
      "source": [
        "from xplique.attributions import (Saliency, GradientInput, IntegratedGradients, SmoothGrad, VarGrad,\n",
        "                                  SquareGrad, GradCAM, Occlusion, Rise, GuidedBackprop,\n",
        "                                  GradCAMPP, Lime, KernelShap, SobolAttributionMethod)\n",
        "\n",
        "# to explain the logits is to explain the class,\n",
        "# to explain the softmax is to explain why this class rather than another\n",
        "# it is therefore recommended to explain the logit\n",
        "model.layers[-1].activation = tf.keras.activations.linear\n",
        "batch_size = 64\n",
        "\n",
        "explainers = [\n",
        "             Saliency(model),\n",
        "             GradientInput(model),\n",
        "             GuidedBackprop(model),\n",
        "             IntegratedGradients(model, steps=50, batch_size=batch_size),\n",
        "             SmoothGrad(model, nb_samples=50, batch_size=batch_size),\n",
        "             SquareGrad(model, nb_samples=50, batch_size=batch_size),\n",
        "             VarGrad(model, nb_samples=50, batch_size=batch_size),\n",
        "             GradCAM(model),\n",
        "             GradCAMPP(model),\n",
        "             Occlusion(model, patch_size=10, patch_stride=10, batch_size=batch_size),\n",
        "             SobolAttributionMethod(model, batch_size=batch_size),\n",
        "             # Rise(model, nb_samples=4000, batch_size=batch_size),\n",
        "             # Lime(model, nb_samples = 1000),\n",
        "             # KernelShap(model, nb_samples = 1000)\n",
        "]\n",
        "\n",
        "explanations_to_test = {}\n",
        "\n",
        "for explainer in explainers:\n",
        "\n",
        "  explainer_name = explainer.__class__.__name__\n",
        "  explanations = explainer(X, Y)\n",
        "\n",
        "  if len(explanations.shape) > 3:\n",
        "    explanations = np.mean(explanations, -1)\n",
        "\n",
        "  # store the explanations to use the metrics\n",
        "  explanations_to_test[explainer_name] = explanations\n",
        "\n",
        "  print(f\"Method: {explainer_name}\")\n",
        "  plot_attributions(explanations[:5], X[:5], cmap='jet', alpha=0.4,\n",
        "                    cols=5, clip_percentile=0.5, absolute_value=True)\n",
        "  plt.show()\n",
        "  print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jsjLCSc7EAB"
      },
      "source": [
        "# üèÖ Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsnwqXZQ7GWR"
      },
      "outputs": [],
      "source": [
        "%load_ext google.colab.data_table\n",
        "import pandas as pd\n",
        "\n",
        "# softmax or sigmoid are needed to correctly compute metrics this time\n",
        "model = app.MobileNetV2()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54QYdvgsCR6w"
      },
      "source": [
        "##¬†**Deletion** Metric\n",
        "\n",
        "The deletion metric measures the drop in the probability of a class as important pixels (given by the saliency map) are gradually removed from the image. A sharp drop, and thus a small area under the probability curve, are indicative of a good explanation.\n",
        "\n",
        "**Lower is better**\n",
        "\n",
        "---\n",
        "\n",
        "Petsiuk & al., RISE: Randomized Input Sampling for Explanation of Black-box Models (2018). https://arxiv.org/pdf/1806.07421.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpgCMDhe_iSJ"
      },
      "outputs": [],
      "source": [
        "from xplique.metrics import Deletion\n",
        "\n",
        "metric = Deletion(model, X, Y, batch_size)\n",
        "deletion_scores = []\n",
        "\n",
        "i = 0\n",
        "for explanation_name, explanations in explanations_to_test.items():\n",
        "  deletion_score = metric(explanations)\n",
        "  deletion_scores.append((explanation_name, deletion_score))\n",
        "\n",
        "pd.DataFrame(deletion_scores, columns=[\"Method name\", \"Deletion score (lower is better)\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHfarWIYKZeC"
      },
      "source": [
        "## **Insertion** Metric\n",
        "\n",
        "The insertion metric, on the other hand, captures the importance of the pixels in terms of their ability to synthesize an image and is measured by the rise in the probability of the class of interest as pixels are added according to the generated importance map.\n",
        "\n",
        "**Higher is better**\n",
        "\n",
        "---\n",
        "\n",
        "Petsiuk & al., RISE: Randomized Input Sampling for Explanation of Black-box Models (2018). https://arxiv.org/pdf/1806.07421.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AA3N7ttTKY-Z"
      },
      "outputs": [],
      "source": [
        "from xplique.metrics import Insertion\n",
        "\n",
        "metric = Insertion(model, X, Y, batch_size)\n",
        "insertion_scores = []\n",
        "\n",
        "i = 0\n",
        "for explanation_name, explanations in explanations_to_test.items():\n",
        "  insertion_score = metric(explanations)\n",
        "  insertion_scores.append((explanation_name, insertion_score))\n",
        "\n",
        "pd.DataFrame(insertion_scores, columns=[\"Method name\", \"Insertion score (higher is better)\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7zvoj2GKk3d"
      },
      "source": [
        "## **MuFidelity** Metric\n",
        "\n",
        "Used to compute the fidelity correlation metric. This metric ensure there is a correlation between a random subset of pixels and their attribution score. For each random subset created, we set the pixels of the subset at a baseline state and obtain the prediction score. This metric measures the correlation between the drop in the score and the importance of the explanation.\n",
        "\n",
        "**Higher is better**\n",
        "\n",
        "---\n",
        "\n",
        "Bhatt & al., Evaluating and Aggregating Feature-based Model Explanations (2020). https://arxiv.org/abs/2005.00631 (def. 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvcQI6JbKnCO"
      },
      "outputs": [],
      "source": [
        "from xplique.metrics import MuFidelity\n",
        "\n",
        "metric = MuFidelity(model, X, Y, batch_size, nb_samples=25)\n",
        "fidelity_scores = []\n",
        "\n",
        "i = 0\n",
        "for explanation_name, explanations in explanations_to_test.items():\n",
        "  fidelity_score = metric(explanations)\n",
        "  fidelity_scores.append((explanation_name, fidelity_score))\n",
        "\n",
        "pd.DataFrame(fidelity_scores, columns=[\"Method name\", \"Fidelity score (higher is better)\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2mm-EXEZysG"
      },
      "source": [
        "## **Stability** Metric\n",
        "\n",
        "Used to compute the average sensitivity metric (or stability). This metric ensure that close inputs with similar predictions yields similar explanations. For each inputs we randomly sample noise to add to the inputs and compute the explanation for the noisy inputs. We then get the average distance between the original explanations and the noisy explanations.\n",
        "\n",
        "**Lower is better**\n",
        "\n",
        "---\n",
        "\n",
        "Bhatt & al., Evaluating and Aggregating Feature-based Model Explanations (2020). https://arxiv.org/abs/2005.00631 (def. 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9_3QlIUcIXM"
      },
      "outputs": [],
      "source": [
        "from xplique.metrics import AverageStability\n",
        "\n",
        "metric = AverageStability(model, X[:20], Y[:20], batch_size, nb_samples=32)\n",
        "stability_scores = []\n",
        "\n",
        "i = 0\n",
        "for explainer in explainers:\n",
        "  explainer_name = explainer.__class__.__name__\n",
        "  stability_score = metric(explainer)\n",
        "  stability_scores.append((explainer_name, stability_score))\n",
        "\n",
        "pd.DataFrame(stability_scores, columns=[\"Method name\", \"Stability score (lower is better)\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-ti7M3KeGek"
      },
      "source": [
        "# üéâ Congratulations\n",
        "**you now know how to use the Attribution Metric module of Xplique !**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "cryptoenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
